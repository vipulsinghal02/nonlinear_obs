\documentclass[psamsfonts]{amsart}
\usepackage{geometry}
\geometry{
  letterpaper,% redundant if already in \documentclass
  % left=15mm,
  % right=15mm,
  top=1in,
  bottom=1in,
  heightrounded,% better use it
}
\usepackage[foot]{amsaddr}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{hyperref}

\setlength{\textwidth}{\paperwidth}
% \addtolength{\textwidth}{-2in}
\addtolength{\textwidth}{-3.5in}
\calclayout

\hypersetup{
pdftitle={A brief tutorial on nonlinear observability},
pdfsubject={System Identification, Differential Geometry, Control Theory, Synthetic Biology},
pdfauthor={Vipul Singhal},
pdfkeywords={system composability, submanifolds, foliations, system Identification, observability, identifiability, differential geometry, nonlinear control}
}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{pdflscape}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} 
\usepackage{mathrsfs}
\usepackage{mathtools} % added from qb16
\usepackage{amssymb}
\usepackage{natbib}

\usepackage{ dsfont } %mathds


\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}
\newtheorem{claim}[thm]{Claim}
\newtheorem{ppty}[thm]{Property}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{limit}[thm]{Limitation}


\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\newcommand*\esp{\theta_e}
\newcommand*\csp{\theta_p} 
\newcommand*\syst{\mathcal{S}}
\newcommand*\extract{\mathcal{E}}
\newcommand*\circuit{\mathcal{P}}
\newcommand*\environment{\mathcal{E}}
\newcommand*\process{\mathcal{P}}
\newcommand*\espset{\mathit{E}} 
\newcommand*\cspset{\mathit{P}}
% \newcommand*\pspset{\mathit{P}}
\newcommand*\thetaset{\Theta}
\newcommand*\R{\mathds{R}}
\newcommand*\calib{\mathrm{cal}}
\newcommand*\test{\mathrm{test}}
\newcommand*\model{{M}}
\newcommand*\experiment{\mathcal{H}}
\newcommand*\textd{\text{d}}
% \usepackage[mathscr]{euscript} 

\newcommand*\nominal[1]{\overline{#1}}
\newcommand*\estimated[1]{\hat{#1}}
\DeclareMathOperator{\id}{{ID}}
\DeclareMathOperator{\proj}{{proj}}
\DeclareMathOperator{\aff}{{cut}}
\DeclareMathOperator{\deriv}{d}
% \newcommand*\modeldata{\mathcal{Y}\circ\mathcal{\model}}
\newcommand*\modeldata{\Gamma}
\newcommand*\placehold{{\color{red}XXYY }}
\newcommand*\isdefined{\triangleq}
\newcommand*\regist{\textsuperscript{\tiny\sffamily\textregistered}}


\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}



\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

%--------Meta Data: Fill in your info------
% \title{A Differential Geometric Approach to Composing Uncertain Systems for Prediction}
% \title{Geometric Properties of Parametric Uncertainty in Genetic Circuits}
% \title{Model outputs foliate parametric uncertainty}
% \title{Notes on nonlinear observability, with applications in uncertainty quantification for systems biology models}
\title{A primer on nonlinear observability and identifiability} % for perturbational control of transcriptional states
\author{Vipul Singhal$^{1}$}
\address{$^1$Genome Institute of Singapore, Agency for Science, Technology and Research, 60 Biopolis St., Singapore, Republic of Singapore}
% \address{$^1$Integrated Biosciences, 900 Island Drive, Redwood City, CA 94065, United States}
\email{vipuls@gis.a-star.edu.sg}
\begin{document}

\begin{abstract}
% {\color{red}Note: these notes should be more or less self contained. Do not expect the reader to look things up while reading. It's okay to expect them to know the general gist of differential geometry, and maybe even have studied it in the past. I.e., these notes serve as a very brief primer or a refresher, so that we can talk about nonlinear control ideas. This is not not a first course in diffgeo. Though it is also not a piece of swiss cheese. Make sure there aren't any major holes! (its okay to skip Hausdorff spaces/the difinition of topological manifolds, second countability, etc. )}

A brief set of notes on nonlinear observability and identifiability. We begin with a description of basic ideas from differential geometry (manifolds, Frobenius theorem, distributions and codistributions), and move on to describing the observability-identifiability codistribution. We end with a method for identifying state and parameter variables that are unobservable or non-identifiable. %This geometric viewpoint is then used to describe uncertainty in models of gene expression in systems and synthetic biology. We also describe the accessibility algebra associated with a perturbed system, which motivates a theory of nonlinear controllability for genetic networks. We end by describing \textit{minimal realizations} of nonlinear control systems, and as a corollary, gene regulatory networks. %and discuss the possibility of controlling the transcriptomic and proteomic state of cells by over and under expressing genes and manipulating signaling pathways. 


% \noindent \textbf{Keywords.} system composability, submanifolds, foliations, system identification, genetic circuits, machine learning, manifold learning
% diffusion maps, polynomial ideals, parameter variety, graph laplacian
\end{abstract}

\maketitle
\tableofcontents

\section{Introduction} 
Nonlinear control is a beautiful area within mathematics, with myriad applications in fields ranging from electrical and aerospace engineering to biology and machine learning (cite). [mention nonlinear observability and controllability]

We briefly review some basic notions from differential geometry: smooth manifolds, coordinate charts, local representatives of objects (maps, vectors, etc.), vectors as differential operators, tangent spaces, and flows generated by vector fields. A more complete treatment can be found in any standard nonlinear control or differntial geometry text, such as \cite{nijmeijer_nonlinear_1990,boothby_2002}. Another excellent source is \cite{abraham_manifolds_1988}, though this is better used as a second course, since its simultaneous focus on depth and breadth can make it feel like one is taking too many detours from the ``goal'' of getting to core results like Frobenius' theorem. 

\section{Differential Geometric Preliminaries}
\subsection{Notational conventions}\label{sec:notationalconventions}
Throughout this text, we will use the Einstein summation convention, where a repeated index, once in a subscript and once in a superscript, implies summation over the full range of values that index can take, obviating the need for a summation symbol. To illustrate this, a vector $v$ expressed as a linear combination of basis vectors $E_1, \dots, E_n$ of a vector space can be written as
\begin{equation}\label{eq:esc}
\begin{aligned}
v & = {} \alpha^i E_i
&\triangleq{} \sum_{i = 1}^n \alpha^i E_i .
\end{aligned}
\end{equation}
A related convention is that vectors, when indexed, will always have subscripts ($E_i$, for instance), while \textit{coefficients}, like $\alpha^i$, will be indexed with superscripts. Similarly, covectors will have superscripts, while the coefficients in their linear expansion will have subscripts, $\omega  = \sum_{i = 1}^n w_i dx^i = w_i dx^i$. Furthermore, the index $i$ in $\frac{\partial}{\partial x^i}$ will be interpreted as a subscript, since it appears in the ``denominator''. In fact, we will regularly abbreviate this partial derivative at a point $a$, that is, $\frac{\partial}{\partial x^i}\bigr|_a$ as $\partial_{ia}$, and indeed, will later consider $\partial_{ia}$ as an element in $T_a\R^n$, the tangent space to $\R^n$ at a point $a\in \R^n$.

\subsection{Manifolds, charts and local representatives}

Roughly speaking, a smooth manifold is a set which is locally Euclidean---in the sense that neighborhoods of points can be modeled using Euclidean spaces---along with a certain \textit{differentiable structure}, which we describe below.\footnote{Our description will omit certain technical details, like the underlying topological space needing to have a second countable basis of open sets, and being Hausdorff. While important in their own right, these details do not usually make an appearance in proofs, and can safely be skipped in a first reading of nonlinear control. The interested reader can find them in any differential geometry text, such as \cite{abraham_manifolds_1988}.} We use the symbol $N$ to denote an $n$-dimensional smooth ($C^{\infty}$) manifold, and the tuples $(U, \varphi)$ and $(V, \phi)$ to denote coordinate charts, where $U$ and $V$ are open sets in $N$ (which exist by virtue of $N$ being a Hausdorff topological space) and the homeomorphisms $\varphi$ and $\phi$ map them (respectively) onto open sets in $\R^n$. The key component of $N$'s differentiable structure is the following compatibility requirement for coordinate changes: whenever $U \cap V$ has a nonempty interior, the map $\phi\circ\varphi^{-1}: \varphi(U\cap V) \rightarrow \phi(U\cap V)$ is a (smooth) diffeomorphism. %This requirement is the critical aspect of what is known as the differentiable structure of a manifold, and turns the topological manifold into a \textit{smooth manifold}. 
% (which is used to define the differentiable structure of $N$, turning the topological manifold into a \textit{smooth manifold})

A point $p\in U$ in local coordinates will typically be denoted by $a = \varphi (p) = (x^1(p), \dots, x^n(p)) \in \R^n$, where the $x^i: U \rightarrow \R$ are called the \textit{coordinate functions} corresponding to a given $\varphi$. Coordinate neighborhoods can therefore also be denoted by $(U, x^1, \dots x^n)$. 

The local representative of a map $f:U\rightarrow \R$ is constructed as $\hat f \triangleq f\circ \varphi^{-1}$, so that 
\begin{align}\label{eq:localrep}
f(p) & = \hat f(\varphi(p)) =\hat f(a) = \hat f(x^1(p), \dots, x^n(p)).
\end{align}
Per convention, we simplify notation by dropping the caret from $f$ and the explicit dependence of the coordinate functions on $p$, resulting in the last two expressions in Equation \ref{eq:localrep} being written $f(a) = f(x^1, \dots, x^n)$. 

\subsection{The Inverse and Implicit Function Theorems, and the Rank Theorem}
In this section, we describe the inverse function theorem, which is one of the foundational results in calculus. We also describe two corollaries---the implicit function and rank theorems---which play an instrumental role in differential geometry. 

Consider two manifolds $N_1$ and $N_2$ of dimension $n_1$ and $n_2$ respectively, with respective coordinate charts $(U, \varphi)$ and $(V, \phi)$ around points $p\in U\subset N_1$ and $F(p)\in V \subset N_2$, where $F:N_1 \rightarrow N_2$ is a map between the manifolds. It's local representative is given by $\hat F: \R^{n_1}\rightarrow \R^{n_2}$, defined by $(x^1, \dots, x_{n_1}) \mapsto \phi \circ F\circ \varphi^{-1}(x^1, \dots, x_{n_1})$. As usual, we will drop the caret, and overload $F$ by using it to refer to both the map between manifolds, and its local representative, since its meaning will be clear from context. $F$ is called a \textit{diffeomorphism} if it is bijective and both (the local representatives of) $F$ and $F^{-1}$ are smooth; if such a diffeomorphism exists, then (i) $n_1=n_2$ and (ii) $N_1$ and $N_2$ are called diffeomorphic. %Note that $\varphi:U\rightarrow \R^{n_1}$ is a diffeomorphism, and this is what it means for a manifold to be locally Euclidean---each $U$ is diffeomorphic to Euclidean space of appropriate dimension. 

\begin{itemize}
\item describe the three theorems
\item can also briefly menion the few propositions (without proof.)
\item and its local representative 
\item submersion
\item immersion
\end{itemize}

\subsection{Vectors as differential operators and the pushforward map}
In this section, we define the tangent space to a manifold, vectors as differential operators on real-valued functions, and the idea of the pushforward of a vector. 

\subsubsection{Tangent Space to $\R^n$}
We begin with defining the \textit{tangent space} to $\R^n$ at $p$, denoted by $T_p\R^n$, in a few steps. Denote by $C^{\infty}(p)$ the set of all real-valued smooth functions whose domain of definition contains a neighbourhood of $p\in \R^n$, {identifying} those functions that agree on an open set containing $p$. Next, let $T_p\R^n$ be an $n$-dimensional vector space, with basis $\{E_{ip}\}_{i=1}^{n}$, and its elements, called the \textit{tangent vectors} at $p$, written as linear combinations of the form (via the summation convention),
\begin{align}\label{eq:tangentvector}
X_p &={} \alpha^i E_{ip},
\end{align}
with coefficients $\alpha^i \in \R$. Tangent vectors of the form \ref{eq:tangentvector} will be identified with (not necessarily unit-norm) directional derivatives of elements of $C^{\infty}(p)$, defined as
\begin{equation}
\begin{aligned}
X_p(f) \triangleq \sum_{i = 1}^n \alpha^i \left(\frac{\partial f}{\partial x^i}\right)_p = \alpha^i \partial_{ip} f,
\end{aligned}
\end{equation}
where the subscript $p$ denotes the point at which the derivative is evaluated. 

Interpreting $X_p$ as a directional derivative means that it can be thought of as a map from $C^{\infty}(p)$ to $\R$. This motivates the definition of a particular type of operator on real-valued smooth functions, called a \textit{derivation}. 

\begin{defn}[Derivation]\label{def:derivations}
The space of \textit{derivations}, denoted $\mathscr{D}(p)$, is the vector space of all mappings $C^{\infty}(p)\rightarrow \R$ such that for all $\alpha, \beta \in \R$ and $f, g \in C^{\infty}(p)$ the following two properties hold:
\begin{equation}\label{eq:linleibniz}
\begin{aligned}
X_p(\alpha f + \beta g) & = {} \alpha (X_p f) + \beta (X_p g) \qquad & \text{(linearity)},\\
X_p(fg) & = {} (X_p f)g(p) + f(p)(X_p g) \qquad & \text{(Leibniz rule)},
\end{aligned}
\end{equation}
with the vector space operations satisfying 
\begin{equation}
\begin{aligned}
(X_p+Y_p)(f) &={} X_p(f) +Y_p(f), \\
(\alpha X_p)(f) &={}\alpha X_p (f), \qquad \alpha \in \R.
 \end{aligned}
 \end{equation}
\end{defn}
It may be verified that directional derivatives satisfy these properties, and are therefore derivations (and in fact, on the algebra of smooth functions $C^{\infty}(p)$, these are known to be the \textit{only} type of derivations to exist.)\footnote{On $C^r(p)$, for $r<\infty$, other types of derivations are known to exist.} These ideas are tied together by the following theorem. 
\begin{thm}
The tangent space $T_p\R^n$ is isomorphic to the vector space $\mathscr{D}(p)$ of all derivations of $C^{\infty}(p)$ into $\R$. This isomorphism is given by making each $X_p$ correspond to the directional derivative in the direction of $X_p$. 
\end{thm}

Using this isomorphism, we can \textit{identify} $T_p\R^n$ with $\mathscr{D}(p)$. We write vectors as linear combinations of basis vectors, $X_p = \alpha^i \partial_{ip}$, so that the $E_{ip}$ are identified with $\partial_{ip}$, and $E_{ip}f = \partial_{ip} f$. Note that $X_p x^i = \alpha^i$, so that $X_p$ is completely determined by its action on the coordinate functions, $x^i$. This equivalence between derivations on functions and the tangent space will be especially useful in defining the tangent space to manifolds, a topic we cover next. 

\subsubsection{Tangent Space to $N$}
Consider a smooth manifold $N$ and a point $p \in N$. We may define $C^{\infty}(p)$ as the algebra of real-valued smooth functions whose domain of definition contains some neighbourhood of $p$ in $N$, with functions identified if they agree on any neighbourhood of $p$. The tangent space to $N$ at $p$ may then be defined using derivations. 
\begin{defn}[Tangent Space to $N$]\label{def:tangentspace}
The \textit{tangent space} to $N$ at point $p$, denoted by $T_pN$, is defined as the space of derivations on $C^{\infty}(p)$, that is, the vector space of mappings $X_p: C^{\infty}(p)\rightarrow \R$ satisfying the linearity and Leibniz rule properties defined in Equation \ref{eq:linleibniz}. Elements of this space, denoted by $X_p\in T_pN$, are called \textit{tangent vectors} at $p$. 
\end{defn} 

Before we can describe elements of $T_pN$ in local coordinates, we need to define two homomorphisms, the pullback of functions and the pushforward of vectors. 

\begin{defn}[Pullback and Pushforward]\label{def:pushforward}
Let $F:N\rightarrow M$ be a $C^{\infty}$ map of manifolds. Then, for $p\in N$ and $f\in C^{\infty}(F(p))$, the \textit{pullback} $F^*:C^{\infty}(F(p))\rightarrow C^{\infty}(p)$ defined by $F^*(f) = f\circ F$ is a homomorphism of algebras. Its dual vector space homomorphism $F_*:T_pN \rightarrow T_{F(p)}M$, is called the \textit{pushforward}, and is defined by $F_*(X_p)f = X_p(F^*f)$, where $F_*(X_p)$ is a map of $C^{\infty}(F(p))$ to $\R$. If $H = G\circ F$, then $H^* = F^* \circ G^*$ and $H_* = G_* \circ F_*$. If $F$ is the identity, then $F^*$ and $F_*$ are the identity isomorphisms (on their respective domains). 
\end{defn}

A corollary of the last statement in Definition \ref{def:pushforward} is that pushforwards created out of diffeomorphisms are isomorphisms (consider the identity isomorphism that is constructed out of the identity map $F^{-1}\circ F$, namely $F_*^{-1}\circ F_*$). Then, for a chart $(U\ni p,\, \varphi)$, the spaces $T_pU$ and $T_{a}\left(\varphi(U)\right) \cong T_{a}\R^n$ are isomorphic, with $a = \varphi(p)$. The basis $ \left\{\partial_{ia}\right\}_{i = 1}^n$ of $T_{a}\R^n$ defines a basis of $T_pU$, 
\begin{equation}
\begin{aligned}
\varphi^{-1}_*&:T_{a}\R^n\rightarrow T_pU, \qquad a = \varphi(p),\\
E_{ip} &\triangleq{} \varphi^{-1}_*\partial_{ia}.
\end{aligned}
\end{equation}
The basis $\{E_{ip}\}_{i=1}^n$ is called a \textit{coordinate frame} of $T_pU$, and is dependent on the choice of $\varphi$. Notice that unlike in the $T_p\R^n$ case discussed previously, here $\{E_{i(\cdot)}\}_{i = 1}^n$ and $\{\partial_{i(\cdot)}\}_{i = 1}^n$ are not \textit{identified} with each other, although are equivalent via isomorphism, which reduces the study of $T_pU$ to the study of $T_{a}\R^n$. Note also that $\partial_i x^j = \delta_{i}^j$, the Kronecker delta. 
% though they are equivalent up to isomorphism, {\color{red}and will be used interchangeably in the sequel) $\leftarrow$ check this later.}. 

With these definitions, we can express $X_pf$ in local coordinates, 
\begin{equation}
\begin{aligned}
X_pf(p) & = {} \alpha^i E_{ip}f(p) \\
& = {} \alpha^i (\varphi_* E_{ip})(\varphi^{-1*}f)(\varphi(p))\\
& = {} \alpha^i \left(\partial_{ia}\hat f\right)(a),%\qquad a = \varphi(p)\\
% & = {} \alpha^i \frac{\partial \hat f}{\partial x^i}(a)
\end{aligned}
\end{equation}
or, dropping the caret, $X_p f(p) = \alpha^i\partial_{ia} f(a)$. 
% Geometrically, we can identify tangent vectors with directional derivatives. Roughly speaking, for $f \in C^{\infty}(p)$, $X_p(f)$ is the derivative of $f$ in the tangent direction $X_p$. Said differently, for a curve $c: (-\epsilon, \epsilon)\rightarrow N$ with $c(0) = p$ and $\frac{dc}{dt}\bigr|_{t=0} = X_p$, we have $X_p(f) = \frac{d(f\circ c)(t)}{dt}\bigr|_{t=0}$.

\subsection{Vector fields, flows, and the Lie derivative}
The tangent bundle associated with a manifold is defined as the disjoint union of individual tangent spaces (that is, keeping track of the base points in addition to the tangent spaces themselves), 
\begin{equation}\label{eq:tangentbundle}
\begin{aligned}
TN & = {} \bigsqcup_{p \in N} T_pN\\
 & = {} \bigcup_{p \in N} \{p\} \times T_pN\\
 & = {} \bigcup_{p \in N} \{(p, v) \mid v \in T_pN\}.
\end{aligned}
\end{equation}
One defines the natural projection $\pi : TN \twoheadrightarrow N : (p, v) \mapsto p$. %, which, in our case, looks like $(p, X_p) \mapsto \pi((p, X_p)) = p$. 
Note that some authors define the tangent bundle without the base point explictly included, $TN = \cup_{p\in N}T_pN$, with projection $v \mapsto \pi(v) = p$ for $v \in T_pN$. Note also that in some cases, it is possible to find an isomorphism between the tangent bundle and the Cartesian product of the base space and any tangent space, such as $T\R^n \cong \R^n \times \R^n$ or $T\mathbb{S}^1 \cong \mathbb{S}^1 \times \R$. In this case, the tangent bundle is called the \textit{trivial bundle}. In general, however, such a decomposition will not exist (the most common example being $T\mathbb{S}^2$), and we will always use the definition in equation \ref{eq:tangentbundle}. The tangent bundle plays an instrumental role in the definition of (smooth) vector fields on manifolds. 

% another way to align. 
% \begin{equation*}\begin{alignedat}{2}
% \text{This line uses has two different points }&= \text{ but also }&&+ \text{are good points to start further lines}\\
% &=\mathrlap{\text{ this line starts with }=}\\
% &&&+ \text{this line should start at }+ \text{ without using hspace}
% \end{alignedat}\end{equation*}

\begin{defn}[Vector Field]\label{def:vectorfield}
A smooth vector field on a smooth manifold $N$ is defined as a smooth map $X:N\rightarrow TN$ satisfying $\pi \circ X = i_N$, where $i_N$ is the identity on $N$. 
\end{defn}
A vector field $X: N \rightarrow TN$ assigns a vector to each point on a manifold, such that the vector lies in the tangent space at that point, and the assignment is \textit{smooth}. Smoothness here is defined as follows. For a chart $(U\ni p, \varphi)$ of $N$, and $\{E_{ip}\}_{i = 1}^n$ the basis of $T_pN$, a vector may be written $X_p = \alpha^i(p) E_{ip}$. If $p$ is varied in $U$, so that we consider different vectors in the vector field, the components $\alpha^i$ are real-valued functions on $N$, which we require to be smooth. In local coordinates, we require 
\begin{equation}\label{eq:vectorfieldcoeff}
\begin{aligned}
\alpha^i = \alpha^i(x^1, \dots, x^n), \qquad i = 1, \dots, n, \quad \text{on}\quad \varphi(U)\subset \R^n
\end{aligned}
\end{equation} 
to be smooth, denoted by $\alpha^i \in C^{\infty}(\varphi(U))$. The requirement $\pi \circ X = i_N$ in Definition \ref{def:vectorfield} is there to ensure that a given vector in the vector field lies in the tangent space with the correct base point.% (which can be ambiguous in the case of trivial bundles; not so much for the definition in \ref{eq:tangentbundle}). %The stipulation $\pi \circ X = i_N$ becomes necessary if one defines $TN$ to be a simple (non-disjoint) union of all tangent spaces, $TN = \cup_{p\in N}T_pN$. 

A coordinate-free definition of vector fields can be given by endowing $TN$ with the structure of a smooth manifold, with $X:N\rightarrow TN$ a smooth mapping. In this scenario, a chart $(U, \varphi) = (U, x^1, \dots, x^n)$ induces a chart $(\pi^{-1}(U), \varphi_*)$ on $TN$. For $X_p\in \pi^{-1}(U)$, expanded as $X_p = \alpha^i E_{ip}$, its local representative in $\varphi_*(\pi^{-1}(U))$ is written as $\hat X_a = \alpha^i \partial_{ia} = (x^1, \dots, x^n, \alpha^1, \dots, \alpha^n)$. One then argues that the natural projection $\pi:TN\rightarrow N$ is a $C^{\infty}$ map between manifolds, and so is $X$ as a map from $N$ into $TN$; see \cite{nijmeijer_nonlinear_1990} and the exercises 5-7 in \cite{boothby_2002} Section IV.2 for details. 

\subsubsection{Flow of a vector field}
Let $X: N \rightarrow TN$ be a vector field on a manifold $N$. Recall the definition of the flow
%\footnote{See \cite{boothby_2002} for a comprehensive discussion of flow maps and one-parameter subgroups, and \cite{nijmeijer_nonlinear_1990} for a more focused definiton.} 
generated by a vector field: given $X$, an initial point $p$ in some neighborhood $U\subset N$, a maximal interval of existence $(a, b)$ of solutions, and a value $t\in (a,b)$, the flow $\phi(t, \cdot): U \rightarrow N$ generated by $X$ maps $p \mapsto \phi(t, p)\in N$, and satisfies the set of differential equations $\partial_t\phi(t, p) = X(\phi(t, p))$.\footnote{{\color{red} Try to improve this flow part via one of the textbooks}}

One may define the derivative of various types of objects with respect to a vector field. The simplest of such objects are $C^{\infty}$ functions, and their Lie derivative along a vector field coincides with the directional derivative defined earlier. 

\begin{defn}[Lie Derivative of a real-valued $C^{\infty}$ function]
Given a real-valued function $f: N \rightarrow \R$, and a vector field $X: N\rightarrow TN$, we define the Lie derivative of $f$ with respect to $X$ as
\begin{align}
L_Xf(p) &\triangleq {} X(f)(p) 
\end{align}
\end{defn}

\begin{equation}
\begin{aligned}
L_Xf(p) &={}\frac{d}{dt}\biggr|_{t=0}\left( f \circ \phi(t, p)  \right)\\
&={} \lim_{h\rightarrow 0}\frac{f(\phi(h, p)-f(p)}{h}
\end{aligned}
\end{equation}
It can be shown that $L_Xh(p)$ is equivalent to $X(p)h$ (the vector $X(p)$ acting as a differential operator---or \textit{derivation}---on $h$ at point $p$), and motivates the alternate notation $L_Xh(p) = X(h)(p)$

Given a chart $(U, \varphi)$ around $p\in N$, we may write $X(p)$ in local coordinates as $(X_1(x), \dots, X_n(x))^T$, where $x = \varphi(p)$



, where a vector field $X$ may be written (with slight abuse of notation) as

 and the Lie derivative might be written 


This local representation illuminates that $X(p)h$, and therefore $L_Xh(p)$, defines a real-valued function $X(h):N\rightarrow \R$. This in turn allows for recursively defined Lie derivatives, $L_{X_{1}}L_{X_{2}}\cdots L_{X_{r}}h$ for some set of vector fields $\{X_1, \dots , X_r\}$.



\subsection{The Lie Bracket, distributions and Frobenius' theorem}\label{sec:prelim-frobenius}


\begin{itemize}
    % \item Lie deriv and flow (first draft done)
    \item lie bracket, distribution, involutivity
    \item frobenius thm
\end{itemize}


\subsection{One-forms, codistributions and pullbacks}
redo in light of the prev sections. also, motivate covectors a little more formally (check Nijmeijer)

Let $N$ denote a \textit{smooth manifold}, $T_pN $ its \textit{tangent space} at point $p \in N$, and $T^*_pN$ the dual \textit{cotangent space}. If $\{x^1, \dots, x^n\}$ are local coordinates around $p$, then
 $\{\left.\frac{\partial}{\partial x^1}\right\rvert_p,\dots, \left.\frac{\partial}{\partial x^n}\right\rvert_p \}$ is a basis for $T_pN$ in local coordinates, with the corresponding dual basis denoted $\{dx^1\vert_p, \dots, dx^n\vert_p\}$, with the defining relationship $dx^i\vert_p \left(\left.\frac{\partial}{\partial x_j}\right\rvert_p\right) = \delta_{ij}$. 
 
 Let $s: N \rightarrow \R$ be a smooth function. At every point $p \in N$, the \textit{differential} of $s$ at $p$ is an element of $T^*_pN$, defined via its action on vectors, 
% , through its action on vectors $X_p \in T_pN$,%. As an element of the dual space, it maps an element $X_p$ of $T_pN$ to a scalar, 
\begin{align}
    ds(p)(X_p) & \isdefined X_p(s)(p) \label{def:differential} \\%\qquad X_p \in T_pN, \\
     & = \left(\sum_{j = 1}^nX_{p, j}\left.\frac{\partial}{\partial x_j}\right\rvert_p\right)s(p) \notag \\
     & = \sum_{i = 1}^n \left.\frac{\partial s}{\partial x^i} \right\rvert_p X_{p, i}, \notag
\end{align}
where we note that vectors are differential operators on real valued smooth functions. 

We can endow $ds(p)$ with a representation in terms of the basis of $T^*_pN$ by considering its action on a basis of $T_pN$, which results\footnote{{\color{red}Consider $ds(p)\fracpartial{ }{x^i} = \fracpartial{}{x^i}s(p)$ }} in $ds(p) =\sum_{i = 1}^n \frac{\partial s}{\partial x^i}(p)dx^i\vert_p$. Then, \eqref{def:differential} can be written as \footnote{Noting the linearity of $dx^i$ and right-distributivity of the expression in the first parentheses.}, $ds(p)(X_p) = \left(\sum_{i = 1}^n \frac{\partial s}{\partial x^i}(p)dx^i  \right)\left(\sum_{j = 1}^nX_{p, j}\frac{\partial}{\partial x_j}\right)$. 

The \textit{cotangent bundle} of a manifold $N$ is defined as $T^*N \isdefined \cup_{p\in N}T_p^*N$, and can be given a manifold structure. Then, a \textit{smooth one-form} $\sigma$ on a smooth manifold $N$ is defined as a smooth map $\sigma : N \rightarrow T^*N$ satisfying $$\pi \circ \sigma = \text{identity (on } N),$$ where $\pi: T^*N \rightarrow N$ is the natural projection. That is, a one-form is a smooth map assigning a cotangent vector to each point $p\in N$.

In local coordinates $x$, one forms may be expressed\footnote{With slight abuses of notation, like dropping the hat from the local representative $\hat \sigma$.} as $\sigma(x) = \sum_{i=1}^n \sigma_i(x)dx^i\vert_{x}$. Being dual objects of vector fields, one-forms act on them as expected: $\sigma(X)(p) = \sigma(p)X(p) \in \R$, and define real-valued smooth functions on $N$, $\sigma(X) : N \rightarrow \R$. Any real-valued smooth function $s$ defines a one-form $ds\in T^*N$ via \eqref{def:differential}. Note the relationship with the Lie derivative, $ds(X) = X(s) = L_Xs$. Note also that not every one form can be written as $ds$ for some real-valued smooth function $s$. However, those that can, are called \textit{exact}. 

\subsection{Pushforwards are derivatives}
Add here. 
\subsection{Relationship between one forms, pushforwards, and differentials}



% {\color{blue} !! verify. time derivative of a real valued function is the pushforward of the unit vector by that function. More generally, if you want to differentiate a $C^{\infty}$ map $F:N \rightarrow Q$ along some direction and at some point, given by the derivation $X_p \in T_pN$, then you simply pushforward this derivation via $F$, as follows: $F_* X_p \in T_{F(p)}Q$. Note that this can be verified by observing that $(F_*X_p)(q)\bigr|_{F(p)} = (X_p)(q\circ F)\bigr|_{p}$. So in the case of time derivatives, let's see what happens. 
% Consider a curve $F:(a, b) \rightarrow N$, the unit vector in $(a,b)$, namely $\partial_t\in T_p(a,b)$ for some $p \in (a, b)$ and its pushforward $F_*\partial_t \in T_{F(p)}N$ defined by 
% \begin{align}
% \left(F_* \partial_t \right)f & =  \partial_t (f\circ F)\\
% & = \frac{\partial f}{\partial s}\biggr|_{s=F(t)}\frac{\partial F}{\partial t}\biggr|_{t},
% \end{align}
% where $f\in C^{\infty}(N)$ is a real-valued smooth function on $N$. Let's consider what this looks like in local coordinates. 

% \begin{align}
% \left(F_* \partial_t \right)f = \partial_t (f\circ F)\biggr|_t & = \frac{\partial f}{\partial s}\biggr|_{s=F(t)}\frac{\partial F}{\partial t}\biggr|_{t}
% & =  
% \end{align}
%   }\\ 

  \subsection{Codistribtutions}




% define distribution and codistribution. 
% Next, we define the dual of a distribution, the (smooth) codistribution. This will be pivotal in our discussion of nonlinear observability and identifiability. 

Recall that a (smooth) distribution {\color{red}add to that section!} assigns a subspace of the tangent space to each point on the manifold in a smooth manner. Similarly, we can define a dual notion, the smooth codistribution. A smooth codistribution assigns, to each point on the manifold, a subspace of the corresponding cotangent space in a smooth manner (to be made precise below). Just as distributions and accessibility algebras play a fundamental role in describing nonlinear controllability, codistributions and \textit{observation spaces} play a similar role in describing nonlinear observability (and by extension, identifiability). We discuss these ideas in the following sections. 

% {\color{red}A \textit{smooth codistribution} $P$ assigns a linear subspace of $T_p^*N$ to each point $p\in N$ in a smooth manner. More precisely,} 
\begin{defn}[Smooth Codistribution]
Around any point $p$, let there exist a neighborhood $U$ of $p$ and a set of smooth one-forms $\sigma_i \in T^*_pN, i\in I$ ($I$ possibly infinite), such that for each $q \in U$, $P(q) = \text{span}\{\sigma_i(q); i \in I\}$. Then $P$ is called a \textit{smooth codistribution }on $N$. 
\end{defn}

In what follows, codistribution will always mean smooth codistribution. A one-form belongs to $P(p)$ if $\sigma(p) \in P(p)$ for any $p\in N$, and a codistribution is constant dimensional if the dimension of $P(p)$ does not depend on $p$. If a codistribution is constant dimensional of dimension $l$, then around each point $p$, there exist $l$ independent one-forms (called the \textit{local generators}) $\sigma_1, \dots, \sigma_l$ such that $P(q) = \text{span}\{\sigma_1(q), \dots, \sigma_l(q)\}$, for $q$ near $p$. 

Next, we define the notions of the kernel and annihilator of a codistribution and distribution respectively. Let $P$ and $D$ be a codistribution and distribution on $N$, respectively. Then, 
\begin{equation}
    \begin{aligned}
        \text{ker}P(p) &={} \{X(p) \mid X \text{ is a vector field s.t. } \sigma(X)(p) = 0, \forall \sigma \in P\}\\
        \text{ann}D(p) &={} \{\sigma(p) \mid \sigma \text{ is a one-form s.t. } \sigma(X)(p) = 0, \forall X \in D\}
    \end{aligned}   
\end{equation}

If $D$ and $P$ are constant dimensional, then $D = \text{ker}( \text{ann} D)$ and $P = \text{ann}(\text{ker} P)$. If $\text{ker} P$ is involutive, then we call $P$ an involutive codistribution. If $P$ is generated by exact one forms, then it is easily shown that it must be involutive. 


\section{Nonlinear Observability}
Consider the nonlinear system given by 
\begin{equation}
    \begin{aligned}
        \dot x & = {} f(x) + \sum_{j = 1}^m g_j u_j, \qquad u = (u_1, \dots, u_m) \in U\subset \R^m), \\
        y_i & = {} h_i(x), \qquad i = 1, \dots, p,
    \end{aligned}\label{eq:system}
\end{equation}
where $h = (h_1, \dots, h_p)^T:N\rightarrow Y=\R^p$ and $y(t, x_0, u) = h(x(t, x_0, u))$

We define the notions of state indistinguishability, local observability, observation space and the observability codistribution. We can then state two versions of the nonlinear observability rank condition. These will allow us to talk about decomposing observability into observable and unobservable modes, analogously to the linear case. These results will be extended to include identifiability using a simple device: including system parameters as additional state variables with zero dynamics, and considering their observability. 

\begin{defn}[Nonlinear observability]
    Two states $x^1, x_2\in N$ are \textit{indistinguishable} for system \eqref{eq:system} if for every admissible input function $u$ the output functions $t\mapsto y(t, x^1, u), t\geq 0$ and $t\mapsto y(t, x_2, u), t\geq 0$ are identical on their common domain of definition. The system is \textit{observable} if the states being indistinguishable implies $x^1=x_2$. 
\end{defn}

\begin{defn}[Nonlinear local observability]
    For an open set $V\subset M$, we say that $x^1, x_2 \in V$ are \textit{$V-$indistinguishable}, denoted $x^1 I^V x_2$ if for every admissible \textit{constant} control $u$ such that $x(t, x^1, u)$ and $x(t, x_2, u)$ remain in $V$ for $t\leq T > 0$, the output functions $y(t, x^1, u)$ and $y(t, x_2, u)$ are equal on their common domain of definition. The system \eqref{eq:system} is called \textit{locally observable} at $x_0$ if there exists a neighborhood $W$ of $x_0$ such that for every neighborhood $V\subset W$ of $x_0$ the relation $x_0 I^V x^1$ implies $x^1 = x_0$. A system is locally observable if it is locally observable for all $x_0 \in M$. 
\end{defn}

A system is locally observable if every state $x_0$ can be distinguished from its neighbors by using system trajectories remaining close to $x_0$. 

\begin{defn}[Observation space]
    The \textit{observation space} $\mathscr{O}$ of the system \eqref{eq:system} is the linear space (over field $\R$) of real valued smooth functions on $N$ containing $h_i, i = 1, \dots, p$ and all repeated Lie derivatives 
    \begin{align}
        L_{X_1}\dots L_{X_k}h_j, \quad j = 1,\dots, p, \quad k = 1,2, \dots
    \end{align}
    with $X_i \in \{f, g_1, \dots, g_m\}$, $i\in\{1, \dots, k\}$. 
\end{defn}

\begin{prop}\label{prop:O-space-equiv}
$\mathscr{O}$ is equivalent to the linear space of functions on $N$ containing $h_1, \dots, h_p$ and all repeated Lie derivatives along system trajectories. These Lie derivatives can be written $L_{Z_1}L_{Z_2}\cdots L_{Z_k}h_j$, with $j \in \{1, \dots, p\}$ and $k=1, 2, \dots$, and $Z_i$, $i\in \{1, \dots, k\}$ of the form
\begin{align}\label{eq:trajvectors}
Z_i(x) = f(x) + \sum_{j = 1}^{m}g_j(x)u_j^i,
\end{align} 
for some point $u^i \in U$. 
\end{prop}
\begin{proof}
The linearity properties of the Lie derivative of a function, $L_{X_1+X_2}H = L_{X_1}H+L_{X_2}H$ and $L_X(H_1+H_2) = L_XH_1 + L_XH_2$, together with the fact that the $Z_i$ are linear combinations of $f, g_1, \dots, g_m$, imply $L_{Z_1}L_{Z_2}\cdots L_{Z_k}h_j \in \mathscr{O}$. Conversely, all vector fields $f, g_1, \dots, g_m$ can be written as linear combinations of vector fields of the form $Z_i$. To see this, note that $f = Z_i$ for $u^i = 0$ for any $i$, and defining $Z_{j}^{+} = f+g_j$ and $Z_{j}^{-} = f-g_j$ (that is, using $u = \pm (0, \dots, 0, 1, 0, \dots, 0)$ in \eqref{eq:trajvectors}, where the $1$ is at the $j$-th coordinate), we have $g_j = \frac{1}{2}(Z_{j}^{+}-Z_{j}^{-})$.  
\end{proof}

Proposition \ref{prop:O-space-equiv} shows that $\mathscr{O}$ comprises the output functions and all derivatives of the output functions along system trajectories. In the case of systems with no inputs (or constant inputs), we can construct $\mathscr{O}$ using $y_j$ and all repeated time derivatives, $\dot y_j = L_fh_j(x)$, $\ddot y = L_fL_fh_j(x)$, and so on. Initially, we will develop our results in this \textit{autonomous} system setting. 

Next, we define the central construct for gauging observability of a system. 
\begin{defn}[Observability Codistribution]
Given $\mathscr{O}$, the \textit{observability codistribution} $d\mathscr{O}$ is defined as
\begin{align}
d\mathscr{O}(q) & = {} \text{span}\{d H(q) \mid H \in \mathscr{O}\}
\end{align}
\end{defn}
Since $d\mathscr{O}$ is generated by exact one-forms, {\color{red}it is involutive}\footnote{{\color{red}Write out involutivity of distributions and codistributions, and the exact one form stuff, frobenius. Also, check what the involutivity of the observability codistribution is needed for.}}. 

\begin{thm}
Consider the system \ref{eq:system} with $\text{dim}N = n$. Assume that $\text{dims}d\mathscr{O}(x_0) = n$. Then the system is locally observable at $x_0$.  
\end{thm}
The 

\bibliographystyle{unsrtnat}
\bibliography{refs}

\end{document}
