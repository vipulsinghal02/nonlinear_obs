\documentclass[psamsfonts]{amsart}
\usepackage{geometry}
\geometry{
  letterpaper,% redundant if already in \documentclass
  % left=15mm,
  % right=15mm,
  top=1in,
  bottom=1in,
  heightrounded,% better use it
}
\usepackage[foot]{amsaddr}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{hyperref}

\setlength{\textwidth}{\paperwidth}
% \addtolength{\textwidth}{-2in}
\addtolength{\textwidth}{-3.5in}
\calclayout

\hypersetup{
pdftitle={A brief tutorial on nonlinear observability},
pdfsubject={System Identification, Differential Geometry, Control Theory, Synthetic Biology},
pdfauthor={Vipul Singhal},
pdfkeywords={system composability, submanifolds, foliations, system Identification, observability, identifiability, differential geometry, nonlinear control}
}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{pdflscape}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} 
\usepackage{mathrsfs}
\usepackage{mathtools} % added from qb16
\usepackage{amssymb}
\usepackage{natbib}

\usepackage{ dsfont } %mathds


\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}
\newtheorem{claim}[thm]{Claim}
\newtheorem{ppty}[thm]{Property}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{limit}[thm]{Limitation}


\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\newcommand*\esp{\theta_e}
\newcommand*\csp{\theta_p} 
\newcommand*\syst{\mathcal{S}}
\newcommand*\extract{\mathcal{E}}
\newcommand*\circuit{\mathcal{P}}
\newcommand*\environment{\mathcal{E}}
\newcommand*\process{\mathcal{P}}
\newcommand*\espset{\mathit{E}} 
\newcommand*\cspset{\mathit{P}}
% \newcommand*\pspset{\mathit{P}}
\newcommand*\thetaset{\Theta}
\newcommand*\R{\mathds{R}}
\newcommand*\calib{\mathrm{cal}}
\newcommand*\test{\mathrm{test}}
\newcommand*\model{{M}}
\newcommand*\experiment{\mathcal{H}}
\newcommand*\textd{\text{d}}
% \usepackage[mathscr]{euscript} 

\newcommand*\nominal[1]{\overline{#1}}
\newcommand*\estimated[1]{\hat{#1}}
\DeclareMathOperator{\id}{{ID}}
\DeclareMathOperator{\proj}{{proj}}
\DeclareMathOperator{\aff}{{cut}}
\DeclareMathOperator{\deriv}{d}
% \newcommand*\modeldata{\mathcal{Y}\circ\mathcal{\model}}
\newcommand*\modeldata{\Gamma}
\newcommand*\placehold{{\color{red}XXYY }}
\newcommand*\isdefined{\triangleq}
\newcommand*\regist{\textsuperscript{\tiny\sffamily\textregistered}}


\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}



\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

%--------Meta Data: Fill in your info------
% \title{A Differential Geometric Approach to Composing Uncertain Systems for Prediction}
% \title{Geometric Properties of Parametric Uncertainty in Genetic Circuits}
% \title{Model outputs foliate parametric uncertainty}
% \title{Notes on nonlinear observability, with applications in uncertainty quantification for systems biology models}
\title{A primer on nonlinear observability and identifiability} % for perturbational control of transcriptional states
\author{Vipul Singhal$^{1}$}
\address{$^1$Genome Institute of Singapore, Agency for Science, Technology and Research, 60 Biopolis St., Singapore, Republic of Singapore}
% \address{$^1$Integrated Biosciences, 900 Island Drive, Redwood City, CA 94065, United States}
\email{vipuls@gis.a-star.edu.sg}
\begin{document}

\begin{abstract}
% {\color{red}Note: these notes should be more or less self contained. Do not expect the reader to look things up while reading. It's okay to expect them to know the general gist of differential geometry, and maybe even have studied it in the past. I.e., these notes serve as a very brief primer or a refresher, so that we can talk about nonlinear control ideas. This is not not a first course in diffgeo. Though it is also not a piece of swiss cheese. Make sure there aren't any major holes! (its okay to skip Hausdorff spaces/the difinition of topological manifolds, second countability, etc. )}

A brief set of notes on nonlinear observability and identifiability. We begin with a description of basic ideas from differential geometry (manifolds, Frobenius theorem, distributions and codistributions), and move on to describing the observability-identifiability codistribution. We end with a method for identifying state and parameter variables that are unobservable or non-identifiable. %This geometric viewpoint is then used to describe uncertainty in models of gene expression in systems and synthetic biology. We also describe the accessibility algebra associated with a perturbed system, which motivates a theory of nonlinear controllability for genetic networks. We end by describing \textit{minimal realizations} of nonlinear control systems, and as a corollary, gene regulatory networks. %and discuss the possibility of controlling the transcriptomic and proteomic state of cells by over and under expressing genes and manipulating signaling pathways. 


% \noindent \textbf{Keywords.} system composability, submanifolds, foliations, system identification, genetic circuits, machine learning, manifold learning
% diffusion maps, polynomial ideals, parameter variety, graph laplacian
\end{abstract}

\maketitle
\tableofcontents

\section{Introduction} 

{\color{red}fix all super / subscripts so that we can switch everything to the Einstein summantion convention.}
Nonlinear control is a beautiful area within mathematics, with myriad applications in fields ranging from electrical and aerospace engineering to biology and machine learning (cite). Add more intro. 

We assume some familiarity with basic notions from differential geometry: smooth manifolds, coordinate charts, local representatives of objects (maps, vectors, etc.), vectors as differential operators, tangent spaces, and flows generated by vector fields. We will briefly review some of these definitions in the next section. A more complete treatment can be found in any standard nonlinear control or differntial geometry text, such as \cite{nijmeijer_nonlinear_1990,boothby_2002}. Another excellent source is \cite{abraham_manifolds_1988}, though this is better used as a second course, since its simultaneous focus on depth and breadth can make it feel like one is taking too many detours from the ``goal'' of getting to core results like Frobenius' theorem. 

\section{Differential Geometric Preliminaries}
\subsection{Manifolds, charts and local representatives}

Roughly speaking, a smooth manifold is a set which is locally Euclidean---in the sense that neighborhoods of points can be modeled using Euclidean spaces---along with a certain \textit{differentiable structure}. We use the symbol $N$ to denote an $n$-dimensional smooth ($C^{\infty}$) manifold, and the tuples $(U, \varphi)$ and $(V, \phi)$ to denote example coordinate charts, where $U$ and $V$ are open sets in $N$ (which exist by virtue of $N$ being a Hausdorff topological space) and the homeomorphisms $\varphi$ and $\phi$ map them (respectively) onto open sets in $\R^n$. The key component of $N$'s differentiable structure is the following compatibility requirement for coordinate changes: whenever $U \cap V$ has a nonempty interior, the map $\phi\circ\varphi^{-1}: \varphi(U\cap V) \rightarrow \phi(U\cap V)$ is a (smooth) diffeomorphism. %This requirement is the critical aspect of what is known as the differentiable structure of a manifold, and turns the topological manifold into a \textit{smooth manifold}. 
% (which is used to define the differentiable structure of $N$, turning the topological manifold into a \textit{smooth manifold})

Given a point $p\in U$, its local representative is given by its local coordinates, $\varphi (p) = (x^1(p), \dots, x^n(p))$, the functions $p \rightarrow x^i(p)$ are called the \textit{coordinate functions}, coordinate neighborhoods can also be denoted $(U, x_1, \dots x_n)$, where we drop the explicit dependence on $p$. The local representative of a map $f:U\rightarrow \R$ is constructed as $\hat f \triangleq f\circ \varphi^{-1}$, so that 
\begin{align}\label{eq:localrep}
f(p) & = \hat f(x_1(p), \dots, x_n(p)).
\end{align}
Per convention, we will drop the caret from $f$ and the explicit dependence of the coordinate functions on $p$, resulting in the RHS of Equation \ref{eq:localrep} being written $f(x_1, \dots, x_n)$. 

\subsection{The Inverse and Implicit Function Theorems, and the Rank Theorem}
In this section, we describe the inverse function theorem, which is one of the pillars on which analysis stands. We also describe two corollaries of the inverse function theorem---the implicit function and rank theorems---which are widely used in calculus and differential geometry. 

Consider two manifolds $N_1$ and $N_2$ of dimension $n_1$ and $n_2$ respectively, with respective coordinate charts $(U, \varphi)$ and $(V, \phi)$ around points $p\in U\subset N_1$ and $F(p)\in V \subset N_2$, where $F:N_1 \rightarrow N_2$ is a map between the manifolds. It's local representative is given by $\hat F(x_1, \dots, x_{n_1}) = F\circ \varphi^{-1} (x_1, \dots, x_{n_1})$, where $(x_1, \dots, x_{n_1}) = \varphi(p)$. As is customary, we will drop the caret, and overload $F$ by using it to refer to both the map between manifolds, and its local representative, and relying on context to make its meaning clear. $F$ is called a \textit{diffeomorphism} if it is bijective and both (the local representatives of) $F$ and $F^{-1}$ are smooth, and if such a diffeomorphism exists then (i) $n_1=n_2$ and (ii) $N_1$ and $N_2$ are called diffeomorphic. Note that $\varphi:U\rightarrow \R^{n_1}$ is a diffeomorphism, and this is what it means for a manifold to be locally Euclidean---each $U$ is diffeomorphic to Euclidean space of appropriate dimension. 

\begin{itemize}
\item describe the three theorems
\item can also briefly menion the few propositions (without proof.)
\item and its local representative 
\item submersion
\item immersion
\end{itemize}

\subsection{Tangent bundles, vectors as operators, and the pushforward map}
In this section, we review the tangent space to a manifold, vectors as differential operators on functions, and the idea of the pushforward of a vector from one manifold to another. 

Denote by $C^{\infty}(p)$ the set of all smooth real-valued functions, whose domain of definition contains a neighbourhood of $p$, \textit{identifying} those functions that agree on an open set containing $p$. Let $T_p\R^n$ be a vector space with basis $\{E_{ip}\}_{i=1}^{n}$, so that a vector may be written, for $\alpha^i \in \R$, $X_p = \sum_{i = 1}^n \alpha^i E_{ip}$. Now define the action of a vector on a real-valued smooth function at $p$ as the directional derivative of that function in the direction of that (not necessarily unit norm) vector, 
\begin{equation}
\begin{aligned}
X_p(f) \triangleq \sum_{i = 1}^n \alpha^i \left(\frac{\partial f}{\partial x^i}\right)_p.
\end{aligned}
\end{equation}
Notice that interpreting $X_p$ as a directional derivative means that it can be thought of as a map from $C^{\infty}(p)$ to $\R$. This motivates us to define a particular type of operator on real-valued smooth functions, called a {derivation}:
\begin{defn}[Derivations]\label{def:derivations}
The space of \textit{derivations}, denoted $\mathscr{D}(p)$, is the vector space of all mappings $C^{\infty}(p)\rightarrow \R$ such that for all $\alpha, \beta \in \R$ and $f, g \in C^{\infty}(p)$ the following two properties hold:
\begin{equation}\label{eq:linleibniz}
\begin{aligned}
X_p(\alpha f + \beta g) & = {} \alpha (X_p f) + \beta (X_p g) \qquad & \text{(linearity)},\\
X_p(fg) & = {} (X_p f)g(p) + f(p)(X_p g) \qquad & \text{(Leibniz rule)},
\end{aligned}
\end{equation}
with the vector space operations satisfying 
\begin{equation}
\begin{aligned}
(X_p+Y_p)(f) &={} X_p(f) +Y_p(f), \\
(\alpha X_p)(f) &={}\alpha X_p (f), \qquad \alpha \in \R.
 \end{aligned}
 \end{equation}
\end{defn}
These ideas are tied together by the following theorem. 
\begin{thm}
The vector space $T_p\R^n$ is isomorphic to the vector space $\mathscr{D}(p)$ of all derivations of $C^{\infty}(p)$ into $\R$. This isomorphism is given by making each $X_p$ correspond to the directional derivative in the direction of $X_p$. 
\end{thm}

Using this isomorphism, we can \textit{identify} $T_p\R^n$ with $\mathscr{D}(p)$. We write vectors as linear combinations of basis vectors, $X_p = \sum_{i = 1}^n \alpha^i \frac{\partial}{\partial x^i}\bigr|_p$, so that the $E_{ip}$ are identified with $\frac{\partial}{\partial x^i}\bigr|_p$, and $E_{ip}f = \frac{\partial f }{ \partial x^i}\bigr|_p$. Note that $X_p x^i = \alpha^i$, so that $X_p$ is completely determined by its action on the coordinate functions, $x^i$. This equivalence between derivations on functions and the tangent space will be especially useful in defining the tangent space to manifolds, a topic we cover next. 

Consider a smooth manifold $N$ and a point $p \in N$. We may define $C^{\infty}(p)$ as the algebra of smooth real-valued functions whose domain of definition contains some neighbourhood of $p$, with functions identified if they agree on any neighbourhood of $p$. The tangent space to $N$ at $p$ may then be defined using derivations. 
\begin{defn}[Tangent Space of $N$]\label{def:tangentspace}
The \textit{tangent space} to $N$ at point $p$, denoted by $T_pN$, is defined as the space of derivations on $C^{\infty}(p)$, i.e., the vector space of mappings $X_p: C^{\infty}(p)\rightarrow \R$ satisfying the linearity and Leibniz rule properties defined in Equation \ref{eq:linleibniz}. Elements of this space, denoted $X_p\in T_pN$, are called \textit{tangent vectors}. 
\end{defn} 

Before we can describe elements of $T_pN$ in local coordinates, we need to define two homomorphisms, the pullback of functions and the pushforward of vectors. 

\begin{defn}[Pullback and Pushforward]\label{def:pushforward}
Let $F:N\rightarrow M$ be a $C^{\infty}$ map of manifolds. Then, for $p\in N$ and $f\in C^{\infty}(F(p)$, the \textit{pullback} $F^*:C^{\infty}(F(p))\rightarrow C^{\infty}(p)$ defined by $F^*(f) = f\circ F$ is a homomorphism of algebras. Its dual vector space homomorphism $F_*:T_pN \rightarrow T_{F(p)}M$, called the \textit{pushforward} and defined by $F_*(X_p)f = X_p(F^*f)$, where $F_*(X_p)$ is a map of $C^{\infty}(F(p))$ to $\R$. If $H = G\circ F$, then $H^* = F^* \circ G^*$ and $H_* = G_* \circ F_*$, and if $F$ is the identity then $F^*$ and $F_*$ are the identity isomorphisms (on their respective domains). 
\end{defn}

A corollary of the last statement in Definition \ref{def:pushforward} is that pushforwards created out of diffeomorphisms are isomorphisms (consider the pushforward from the identity $F^{-1}\circ F$, that is, the identity isomorphism $F_*^{-1}\circ F_*$). Then, for a given coordinate chart $(U, \varphi)$ around $p\in N$, the spaces $T_pU$ and $T_{a}\R^n$ are isomorphic, with $a = \varphi(p)$. The basis $ \left\{\frac{\partial }{\partial x^i}\right\}_{i = 1}^n\subset T_{a}\R^n$ defines a basis of $T_pU$ as follows, 
\begin{equation}
\begin{aligned}
\varphi^{-1}_*&:T_{a}\R^n\rightarrow T_pU, \qquad a = \varphi(p)\\
E_{ip} &={} \varphi^{-1}_*\left(\frac{\partial}{\partial x^i}\biggr|_a\right) 
\end{aligned}
\end{equation}
The basis $\left\{E_{ip}\right\}_{i=1}^n$ so defined is called a \textit{coordinate frame} of $T_pU$. Notice that unlike in the $T_p\R^n$ case discussed previously, here we distinguish between the $E_{i(\cdot)}$ and $\frac{\partial}{\partial x^i}$, although these are equivallent up to isomorphism. In fact, this isomorphism allows us to reduce the study of $T_pU$ to the study of $T_{a}\R^n$. Note also that $\frac{\partial}{\partial x^i}x^j = \delta_{i}^j$, the Kronecker delta. 
% though they are equivalent up to isomorphism, {\color{red}and will be used interchangeably in the sequel) $\leftarrow$ check this later.}. 

% Geometrically, we can identify tangent vectors with directional derivatives. Roughly speaking, for $f \in C^{\infty}(p)$, $X_p(f)$ is the derivative of $f$ in the tangent direction $X_p$. Said differently, for a curve $c: (-\epsilon, \epsilon)\rightarrow N$ with $c(0) = p$ and $\frac{dc}{dt}\bigr|_{t=0} = X_p$, we have $X_p(f) = \frac{d(f\circ c)(t)}{dt}\bigr|_{t=0}$.

\subsection{Vector Fields and Distributions}
The tangent bundle associated with a manifold is defined as the disjoint union of individual tangent spaces (i.e., keeping track of the base points in addition to the tangent spaces themselves), 
\begin{equation}
\begin{aligned}
TN & = {} \bigsqcup_{p \in N} T_pN\\
 & = {} \bigcup_{p \in N} \{p\} \times T_pN\\
 & = {} \bigcup_{p \in N} \{(p, v) \mid v \in T_pN\}.
\end{aligned}
\end{equation}
One defines the natural projection $\pi : TN \twoheadrightarrow N : (p, X_p) \mapsto p$. %, which, in our case, looks like $(p, X_p) \mapsto \pi((p, X_p)) = p$. 
Note that some authors define the tangent bundle without the base point explictly included, $TN = \cup_{p\in N}T_pN$, with projection $X_p \mapsto \pi(X_p) = p$. The tangent bundle is useful when defining (smooth) vector fields on manifolds. 

% another way to align. 
% \begin{equation*}\begin{alignedat}{2}
% \text{This line uses has two different points }&= \text{ but also }&&+ \text{are good points to start further lines}\\
% &=\mathrlap{\text{ this line starts with }=}\\
% &&&+ \text{this line should start at }+ \text{ without using hspace}
% \end{alignedat}\end{equation*}
A vector field $X: N \rightarrow TN$ assigns, in a smooth manner, a vector to each point on a manifold. Smoothness of this assignment is defined as follows. For $(U, \varphi)$ a chart around a point $p\in N$, and $\{E_{ip}\}_{i = 1}^n$ the basis of $T_pN$, a vector may be written $X_p = \sum_{i = 1}^n \alpha^i(p) E_{ip}$. If $p$ is varied in $U$, so that we consider different vectors in the vector field, the components $\alpha^i$ are real-valued functions on $N$, which we require to be smooth. In local coordinates, we require 
\begin{equation}\label{eq:vectorfieldcoeff}
\begin{aligned}
\alpha^i = \alpha^i(x^1, \dots, x^n), \qquad i = 1, \dots, n, \quad \text{on}\quad \varphi(U)\subset \R^n
\end{aligned}
\end{equation} 
to be smooth, denoted by $\alpha^i \in C^{\infty}(\varphi(U))$. Note also that $\pi \circ X = i_N$, where $i_N$ is the identity on $N$. 

Another very elegant coordinate-free definition of vector fields can be given by endowing $TN$ with the structure of a smooth manifold, with $X:N\rightarrow TN$ a smooth mapping. For each chart $(U, \varphi)$, with local coordinates $x^1, \dots, x^n$ and basis vectors 

, we define a chart $(\tilde U, \tilde \varphi)$ as follows. 


This is the approach taken by \cite{nijmeijer_nonlinear_1990}, and I encourage the reader to have a look. 

\subsection{Lie Derivatives, distributions and Frobenius' Theorem}\label{sec:prelim-frobenius}

{\color{red}
\begin{itemize}
    \item vector field (briefly), local representative, so you dont need to define it for the lie derivative below. 
    \item Lie deriv and flow (first draft done)
    \item lie bracket, involutivity
    \item frobenius thm
\end{itemize}}

Let $X: N \rightarrow TN$ be a vector field on a manifold $N$, where $TN$ is the tangent bundle on $N$. Recall the definition of the flow
%\footnote{See \cite{boothby_2002} for a comprehensive discussion of flow maps and one-parameter subgroups, and \cite{nijmeijer_nonlinear_1990} for a more focused definiton.} 
generated by a vector field: given $X$, an initial point $p$ in some neighborhood $U\subset N$, a maximal interval of existence $(a, b)$ of solutions, and a value $t\in (a,b)$, the flow $\phi(t, \cdot): U \rightarrow N$ generated by $X$ maps $p \mapsto \phi(t, p)\in N$, and satisfies the set of differential equations $d\phi(t, p)/dt = X(\phi(t, p))$.\footnote{{\color{red} Try to improve this flow part via one of the textbooks}}

\begin{defn}[Lie Derivative of a real valued function]
Given a real-valued function $h: N \rightarrow \R$, and a vector field $X: N\rightarrow TN$, we define the Lie derivative of $h$ with respect to $X$ as
\begin{align}
L_Xh(p) \triangleq \frac{d}{dt}\biggr|_{t=0}\left( h \circ \phi(t, p)  \right), 
\end{align}
\end{defn}
It can be shown that $L_Xh(p)$ is equivalent to $X(p)h$ (the vector $X(p)$ acting as a differential operator---or \textit{derivation}---on $h$ at point $p$), and motivates the alternate notation $L_Xh(p) = X(h)(p)$

Given a chart $(U, \varphi)$ around $p\in N$, we may write $X(p)$ in local coordinates as $(X_1(x), \dots, X_n(x))^T$, where $x = \varphi(p)$



, where a vector field $X$ may be written (with slight abuse of notation) as

 and the Lie derivative might be written 
\begin{align}
L_Xh(p) = X(h)(p) = \lim_{h\rightarrow 0}\frac{f(\phi(h, p)-f(p)}{h}{\color{red} stop here. }
\end{align}

This local representation illuminates that $X(p)h$, and therefore $L_Xh(p)$, defines a real-valued function $X(h):N\rightarrow \R$. This in turn allows for recursively defined Lie derivatives, $L_{X_{1}}L_{X_{2}}\cdots L_{X_{r}}h$ for some set of vector fields $\{X_1, \dots , X_r\}$.


\subsection{One-forms, codistributions and pullbacks}
Let $N$ denote a \textit{smooth manifold}, $T_pN $ its \textit{tangent space} at point $p \in N$, and $T^*_pN$ the dual \textit{cotangent space}. If $\{x_1, \dots, x_n\}$ are local coordinates around $p$, then
 $\{\left.\frac{\partial}{\partial x_1}\right\rvert_p,\dots, \left.\frac{\partial}{\partial x_n}\right\rvert_p \}$ is a basis for $T_pN$ in local coordinates, with the corresponding dual basis denoted $\{dx_1\vert_p, \dots, dx_n\vert_p\}$, with the defining relationship $dx_i\vert_p \left(\left.\frac{\partial}{\partial x_j}\right\rvert_p\right) = \delta_{ij}$. 
 
 Let $s: N \rightarrow \R$ be a smooth function. At every point $p \in N$, the \textit{differential} of $s$ at $p$ is an element of $T^*_pN$, defined via its action on vectors, 
% , through its action on vectors $X_p \in T_pN$,%. As an element of the dual space, it maps an element $X_p$ of $T_pN$ to a scalar, 
\begin{align}
    ds(p)(X_p) & \isdefined X_p(s)(p) \label{def:differential} \\%\qquad X_p \in T_pN, \\
     & = \left(\sum_{j = 1}^nX_{p, j}\left.\frac{\partial}{\partial x_j}\right\rvert_p\right)s(p) \notag \\
     & = \sum_{i = 1}^n \left.\frac{\partial s}{\partial x_i} \right\rvert_p X_{p, i}, \notag
\end{align}
where we note that vectors are differential operators on real valued smooth functions. 

We can endow $ds(p)$ with a representation in terms of the basis of $T^*_pN$ by considering its action on a basis of $T_pN$, which results\footnote{{\color{red}Consider $ds(p)\fracpartial{ }{x_i} = \fracpartial{}{x_i}s(p)$ }} in $ds(p) =\sum_{i = 1}^n \frac{\partial s}{\partial x_i}(p)dx_i\vert_p$. Then, \eqref{def:differential} can be written as \footnote{Noting the linearity of $dx_i$ and right-distributivity of the expression in the first parentheses.}, $ds(p)(X_p) = \left(\sum_{i = 1}^n \frac{\partial s}{\partial x_i}(p)dx_i  \right)\left(\sum_{j = 1}^nX_{p, j}\frac{\partial}{\partial x_j}\right)$. 

The \textit{cotangent bundle} of a manifold $N$ is defined as $T^*N \isdefined \cup_{p\in N}T_p^*N$, and can be given a manifold structure. Then, a \textit{smooth one-form} $\sigma$ on a smooth manifold $N$ is defined as a smooth map $\sigma : N \rightarrow T^*N$ satisfying $$\pi \circ \sigma = \text{identity (on } N),$$ where $\pi: T^*N \rightarrow N$ is the natural projection. That is, a one-form is a smooth map assigning a cotangent vector to each point $p\in N$.

In local coordinates $x$, one forms may be expressed\footnote{With slight abuses of notation, like dropping the hat from the local representative $\hat \sigma$.} as $\sigma(x) = \sum_{i=1}^n \sigma_i(x)dx_i\vert_{x}$. Being dual objects of vector fields, one-forms act on them as expected: $\sigma(X)(p) = \sigma(p)X(p) \in \R$, and define smooth real-valued functions on $N$, $\sigma(X) : N \rightarrow \R$. Any smooth real-valued function $s$ defines a one-form $ds\in T^*N$ via \eqref{def:differential}. Note the relationship with the Lie derivative, $ds(X) = X(s) = L_Xs$. Note also that not every one form can be written as $ds$ for some smooth real-valued function $s$. However, those that can, are called \textit{exact}. 

\subsection{Pushforwards are derivatives}
Add here. 
\subsection{Relationship between one forms, pushforwards, and differentials}



{\color{blue} !! verify. time derivative of a real valued function is the pushforward of the unit vector by that function. More generally, if you want to differentiate a $C^{\infty}$ map $F:N \rightarrow Q$ along some direction and at some point, given by the derivation $X_p \in T_pN$, then you simply pushforward this derivation via $F$, as follows: $F_* X_p \in T_{F(p)}Q$. Note that this can be verified by observing that $(F_*X_p)(q)\bigr|_{F(p)} = (X_p)(q\circ F)\bigr|_{p}$. So in the case of time derivatives, let's see what happens. 
Consider a curve $F:(a, b) \rightarrow N$, the unit vector in $(a,b)$, namely $\partial_t\in T_p(a,b)$ for some $p \in (a, b)$ and its pushforward $F_*\partial_t \in T_{F(p)}N$ defined by 
\begin{align}
\left(F_* \partial_t \right)f & =  \partial_t (f\circ F)\\
& = \frac{\partial f}{\partial s}\biggr|_{s=F(t)}\frac{\partial F}{\partial t}\biggr|_{t},
\end{align}
where $f\in C^{\infty}(N)$ is a real-valued smooth function on $N$. Let's consider what this looks like in local coordinates. 

\begin{align}
\left(F_* \partial_t \right)f = \partial_t (f\circ F)\biggr|_t & = \frac{\partial f}{\partial s}\biggr|_{s=F(t)}\frac{\partial F}{\partial t}\biggr|_{t}
& =  
\end{align}
  }\\ 

  \subsection{Codistribtutions}




% define distribution and codistribution. 
% Next, we define the dual of a distribution, the (smooth) codistribution. This will be pivotal in our discussion of nonlinear observability and identifiability. 

Recall that a (smooth) distribution {\color{red}add to that section!} assigns a subspace of the tangent space to each point on the manifold in a smooth manner. Similarly, we can define a dual notion, the smooth codistribution. A smooth codistribution assigns, to each point on the manifold, a subspace of the corresponding cotangent space in a smooth manner (to be made precise below). Just as distributions and accessibility algebras play a fundamental role in describing nonlinear controllability, codistributions and \textit{observation spaces} play a similar role in describing nonlinear observability (and by extension, identifiability). We discuss these ideas in the following sections. 

% {\color{red}A \textit{smooth codistribution} $P$ assigns a linear subspace of $T_p^*N$ to each point $p\in N$ in a smooth manner. More precisely,} 
\begin{defn}[Smooth Codistribution]
Around any point $p$, let there exist a neighborhood $U$ of $p$ and a set of smooth one-forms $\sigma_i \in T^*_pN, i\in I$ ($I$ possibly infinite), such that for each $q \in U$, $P(q) = \text{span}\{\sigma_i(q); i \in I\}$. Then $P$ is called a \textit{smooth codistribution }on $N$. 
\end{defn}

In what follows, codistribution will always mean smooth codistribution. A one-form belongs to $P(p)$ if $\sigma(p) \in P(p)$ for any $p\in N$, and a codistribution is constant dimensional if the dimension of $P(p)$ does not depend on $p$. If a codistribution is constant dimensional of dimension $l$, then around each point $p$, there exist $l$ independent one-forms (called the \textit{local generators}) $\sigma_1, \dots, \sigma_l$ such that $P(q) = \text{span}\{\sigma_1(q), \dots, \sigma_l(q)\}$, for $q$ near $p$. 

Next, we define the notions of the kernel and annihilator of a codistribution and distribution respectively. Let $P$ and $D$ be a codistribution and distribution on $N$, respectively. Then, 
\begin{equation}
    \begin{aligned}
        \text{ker}P(p) &={} \{X(p) \mid X \text{ is a vector field s.t. } \sigma(X)(p) = 0, \forall \sigma \in P\}\\
        \text{ann}D(p) &={} \{\sigma(p) \mid \sigma \text{ is a one-form s.t. } \sigma(X)(p) = 0, \forall X \in D\}
    \end{aligned}   
\end{equation}

If $D$ and $P$ are constant dimensional, then $D = \text{ker}( \text{ann} D)$ and $P = \text{ann}(\text{ker} P)$. If $\text{ker} P$ is involutive, then we call $P$ an involutive codistribution. If $P$ is generated by exact one forms, then it is easily shown that it must be involutive. 


\section{Nonlinear Observability}
Consider the nonlinear system given by 
\begin{equation}
    \begin{aligned}
        \dot x & = {} f(x) + \sum_{j = 1}^m g_j u_j, \qquad u = (u_1, \dots, u_m) \in U\subset \R^m), \\
        y_i & = {} h_i(x), \qquad i = 1, \dots, p,
    \end{aligned}\label{eq:system}
\end{equation}
where $h = (h_1, \dots, h_p)^T:N\rightarrow Y=\R^p$ and $y(t, x_0, u) = h(x(t, x_0, u))$

We define the notions of state indistinguishability, local observability, observation space and the observability codistribution. We can then state two versions of the nonlinear observability rank condition. These will allow us to talk about decomposing observability into observable and unobservable modes, analogously to the linear case. These results will be extended to include identifiability using a simple device: including system parameters as additional state variables with zero dynamics, and considering their observability. 

\begin{defn}[Nonlinear observability]
    Two states $x_1, x_2\in N$ are \textit{indistinguishable} for system \eqref{eq:system} if for every admissible input function $u$ the output functions $t\mapsto y(t, x_1, u), t\geq 0$ and $t\mapsto y(t, x_2, u), t\geq 0$ are identical on their common domain of definition. The system is \textit{observable} if the states being indistinguishable implies $x_1=x_2$. 
\end{defn}

\begin{defn}[Nonlinear local observability]
    For an open set $V\subset M$, we say that $x_1, x_2 \in V$ are \textit{$V-$indistinguishable}, denoted $x_1 I^V x_2$ if for every admissible \textit{constant} control $u$ such that $x(t, x_1, u)$ and $x(t, x_2, u)$ remain in $V$ for $t\leq T > 0$, the output functions $y(t, x_1, u)$ and $y(t, x_2, u)$ are equal on their common domain of definition. The system \eqref{eq:system} is called \textit{locally observable} at $x_0$ if there exists a neighborhood $W$ of $x_0$ such that for every neighborhood $V\subset W$ of $x_0$ the relation $x_0 I^V x_1$ implies $x_1 = x_0$. A system is locally observable if it is locally observable for all $x_0 \in M$. 
\end{defn}

A system is locally observable if every state $x_0$ can be distinguished from its neighbors by using system trajectories remaining close to $x_0$. 

\begin{defn}[Observation space]
    The \textit{observation space} $\mathscr{O}$ of the system \eqref{eq:system} is the linear space (over field $\R$) of real valued smooth functions on $N$ containing $h_i, i = 1, \dots, p$ and all repeated Lie derivatives 
    \begin{align}
        L_{X_1}\dots L_{X_k}h_j, \quad j = 1,\dots, p, \quad k = 1,2, \dots
    \end{align}
    with $X_i \in \{f, g_1, \dots, g_m\}$, $i\in\{1, \dots, k\}$. 
\end{defn}

\begin{prop}\label{prop:O-space-equiv}
$\mathscr{O}$ is equivalent to the linear space of functions on $N$ containing $h_1, \dots, h_p$ and all repeated Lie derivatives along system trajectories. These Lie derivatives can be written $L_{Z_1}L_{Z_2}\cdots L_{Z_k}h_j$, with $j \in \{1, \dots, p\}$ and $k=1, 2, \dots$, and $Z_i$, $i\in \{1, \dots, k\}$ of the form
\begin{align}\label{eq:trajvectors}
Z_i(x) = f(x) + \sum_{j = 1}^{m}g_j(x)u_j^i,
\end{align} 
for some point $u^i \in U$. 
\end{prop}
\begin{proof}
The linearity properties of the Lie derivative of a function, $L_{X_1+X_2}H = L_{X_1}H+L_{X_2}H$ and $L_X(H_1+H_2) = L_XH_1 + L_XH_2$, together with the fact that the $Z_i$ are linear combinations of $f, g_1, \dots, g_m$, imply $L_{Z_1}L_{Z_2}\cdots L_{Z_k}h_j \in \mathscr{O}$. Conversely, all vector fields $f, g_1, \dots, g_m$ can be written as linear combinations of vector fields of the form $Z_i$. To see this, note that $f = Z_i$ for $u^i = 0$ for any $i$, and defining $Z_{j}^{+} = f+g_j$ and $Z_{j}^{-} = f-g_j$ (i.e., using $u = \pm (0, \dots, 0, 1, 0, \dots, 0)$ in \eqref{eq:trajvectors}, where the $1$ is at the $j$-th coordinate), we have $g_j = \frac{1}{2}(Z_{j}^{+}-Z_{j}^{-})$.  
\end{proof}

Proposition \ref{prop:O-space-equiv} shows that $\mathscr{O}$ comprises the output functions and all derivatives of the output functions along system trajectories. In the case of systems with no inputs (or constant inputs), we can construct $\mathscr{O}$ using $y_j$ and all repeated time derivatives, $\dot y_j = L_fh_j(x)$, $\ddot y = L_fL_fh_j(x)$, and so on. Initially, we will develop our results in this \textit{autonomous} system setting. 

Next, we define the central construct for gauging observability of a system. 
\begin{defn}[Observability Codistribution]
Given $\mathscr{O}$, the \textit{observability codistribution} $d\mathscr{O}$ is defined as
\begin{align}
d\mathscr{O}(q) & = {} \text{span}\{d H(q) \mid H \in \mathscr{O}\}
\end{align}
\end{defn}
Since $d\mathscr{O}$ is generated by exact one-forms, {\color{red}it is involutive}\footnote{{\color{red}Write out involutivity of distributions and codistributions, and the exact one form stuff, frobenius. Also, check what the involutivity of the observability codistribution is needed for.}}. 

\begin{thm}
Consider the system \ref{eq:system} with $\text{dim}N = n$. Assume that $\text{dims}d\mathscr{O}(x_0) = n$. Then the system is locally observable at $x_0$.  
\end{thm}
The 

\bibliographystyle{unsrtnat}
\bibliography{refs}

\end{document}
